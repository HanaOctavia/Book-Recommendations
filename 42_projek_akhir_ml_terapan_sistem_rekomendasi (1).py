# -*- coding: utf-8 -*-
"""42_Projek_Akhir_ML_Terapan_Sistem_Rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vHnsH93AmJXd3zpEQYAv7MqybukSUCm

# Membuat Sistem Rekomendasi Buku

# Data Understanding

**Unzip Dataset**

Pada tahap ini kita akan melakukan unzip, terhadap dataset yang sudah kita upload
"""

!unzip /content/booksdataset.zip

"""**Read Data dan lihat jumlah data**

Pada tahap ini lakukan read dataset dengan fungsi read_csv() dan menghitung jumlah data unik dengan fungsi len()
"""

import pandas as pd

books = pd.read_csv('/content/booksdataset/Books.csv')
user = pd.read_csv('/content/booksdataset/Users.csv')
rating = pd.read_csv('/content/booksdataset/Ratings.csv')

print('Jumlah data buku: ', len(books.ISBN.unique()))
print('Jumlah data user: ', len(user.UserID.unique()))
print('Jumlah data rating: ', len(rating.UserID.unique()))

"""# Univariate Exploratory Data Analysis

**Exploratory variabel books**

Melihat variabel book secara keseluruhan
"""

books

"""Melihat variabel dan tipe data setiap variabel yang ada di dalam variabel books"""

books.info()

"""Menghitung jumlah buku yang unik dihitung lewat ISBN buku dan menampilkan nama-nama buku tersebut"""

print('Banyak data: ', len(books.ISBN.unique()))
print('nama buku: ', books.BookTitle.unique())

"""**Exploratory variabel user**

Melihat data user secara keseluruhan
"""

user

"""mengetahui variabel apa saja yang ada di dalam variabel user dan tipe data variabel tersebut"""

user.info()

"""**Exploratory Variabel Rating**

melihat 5 data teratas variabel rating
"""

rating.head()

"""Tahap di bawah ini kita mengekspolarasi variabel rating dengan melihat variabel apa saja yang ada di dalam variabel rating beserta tipe datanya"""

rating.info()

"""Menghitung jumlah data rating berdasarkan userID dan melihat rating berapa saja yang sudah diberikan pembaca kepada sebuah buku"""

print('Banyak User: ', len(rating.UserID.unique()))
print('Data Rating : ', rating.BookRating.unique())

"""# Data Preprocessing

**Menggabungkan dataframe rating dengan books berdasarkan ISBN**

Pada tahap ini kita akan menggabungkan 2 variabel yaitu rating dan books menjadi sebuah dataframe baru yaitu books berdasarkan ISBN menggunakan fungsi merge()
"""

books = pd.merge(rating, books , on='ISBN', how='left')
books

"""# Data Preparation

**Mengecek missing value**

Menggunakan fungsi isnull() untuk mengecek null value pada dataframe yang sudah digabung tadi kemudian menjumlahkannya dengan fungsi sum()
"""

books.isnull().sum()

"""**Mengatasi missing value**

Setelah di cek 7, variabel terdeteksi null valueBookTitle, BookAuthor, YearOfPublication, Publisher, ImageURLS, ImageURLM, ImageURLL  sebanyak 107467.

Lakukan pembersihan missing value dengan fungsi drop.na()
"""

books_clean = books.dropna()
books_clean

"""Mengecek berapa jumlah buku dengan fungsi len()"""

len(books_clean.ISBN.unique())

"""Melihat data judul buku yang unik"""

books_clean.BookTitle.unique()

"""**Menghitung dan Melihat Berapa kali buku di rating oleh pembaca**

Pada tahap ini kita akan menghitung berapa kali sebuah buku dirating oleh pembaca dengan fungsi count() dan mengelompolannya berdasarkan judul buku dengan fungsi groupby()
"""

jml_rating=books_clean.groupby('BookTitle').count()['BookRating'].reset_index()
jml_rating.rename(columns={'BookRating':'jumlah_rating'},inplace=True)
jml_rating

"""**Menghitung dan Melihat Berapa kali buku di rating oleh pembaca**

Pada tahap ini kita akan menghitung rata-rata rating dengan fungsi mean() dan mengelompokannya berdasarkan judul buku dengan fungsi gropby()
"""

rt_rating=books_clean.groupby('BookTitle').mean()['BookRating'].reset_index()
rt_rating

"""**Menggabungkan dataframe jml_rating dengan dataframe rt_rating **

Menggabungkan kedua variabel di atas yaitu jml_rating dengan rt_rating berdasarkan book title, yang ditambung dalam dataframe baru yaitu popular_books
"""

popular_books= jml_rating.merge(rt_rating,on = 'BookTitle')
popular_books

"""**Mengambil data buku yang populer sebanyak 50**

Tahap di bawah ini kita mengambil jumlah rating yang bernilai >= 500 kemudian kita urutkan berdasarkan rating bukunya.
"""

popular_books=popular_books[popular_books['jumlah_rating']>=500].sort_values('BookRating',ascending=False).head(50)

"""**Merge popular_books dengan dataframe books**

Gabungkan kembali data popular books diatas dengan dataset books awal kemudian drop booktitle yang duplikat
"""

popular_books=popular_books.merge(books,on='BookTitle').drop_duplicates('BookTitle')

"""Melihat data popular_books secara keseluruhan"""

popular_books

"""**Mengkonversi data series menjadi list**

Pada tahap ini kita akan mengkonversikan 3 data series yaitu ISBN, BookTitle dan BookAuthor menjadi list
"""

book_id = popular_books['ISBN'].tolist()
 
book_title = popular_books['BookTitle'].tolist()

book_author = popular_books['BookAuthor'].tolist()
 
print(len(book_id))
print(len(book_title))
print(len(book_author))

"""**Membuat dictionary untuk menentukan pasangan key-value pada data**

Membuat dictionary untuk ketiga data series yang sudah diubah menjadi list dan menenukan pasangan key-value
"""

book_new = pd.DataFrame({
    'id': book_id,
    'book_name': book_title,
    'author': book_author
})
book_new

"""# Model Development dengan Content Based Filtering

**TF-IDF Vectorizer**

Pada tahap ini, kita akan membangun sistem rekomendasi sederhana berdasarkan penulis buku. Langkah-langkah yang dilakukan adalah
- Inisialisasi TfidfVectorizer
- Melakukan perhitungan idf pada data author
- Mapping array dari fitur index integer ke fitur nama
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 

tf = TfidfVectorizer()
 
tf.fit(book_new['author']) 
 
tf.get_feature_names()

"""**Melakukan fit dan transformasi ke dalam bentuk matriks**

Pada tahap ini kita Melakukan fit lalu ditransformasikan ke bentuk matrix, kemudian kita akan mengecek ukuran matrix tfidf
"""

tfidf_matrix = tf.fit_transform(book_new['author']) 
 
tfidf_matrix.shape

"""**Menghasilkan vektor tf-idf dalam bentuk matriks**

Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
"""

tfidf_matrix.todense()

"""**Melihat matriks tf-idf untuk beberapa judul buku dan nama author**

Melihat persebaran matriks tf-idf untuk beberapa judul buku dan nama author pada sebuah tabel
"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=book_new.book_name
).sample(22, axis=1).sample(10, axis=0)

"""**Menghitung derajat kesamaan**

Pada tahap ini kita akan menghitung cosine similarity pada matrix tf-idf
"""

from sklearn.metrics.pairwise import cosine_similarity
 
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""**Melihat matriks kesamaan setiap buku**

Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama buku, kemudain mengecek similarity matrix pada setiap buku
"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=book_new['book_name'], columns=book_new['book_name'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Mendapatkan Rekomendasi**"""

def book_recommendations(book_name, similarity_data=cosine_sim_df, items=book_new[['book_name', 'author']], k=5):
     

    index = similarity_data.loc[:,book_name].to_numpy().argpartition(
        range(-1, -k, -1))
    
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    closest = closest.drop(book_name, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

book_new[book_new.book_name.eq('The Firm')]

book_recommendations('The Firm')

"""# Model Development dengan Collaborative Filtering

## Data Understanding
"""

# Import library
import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""**Membaca dataset**"""

df = rating
df

"""## Data Preparation

**menyandikan (encode) fitur ‘user’ dan ‘ISBN’ ke dalam indeks integer**

Pada tahap ini kit mengubah userID menjadi list tanpa nilai yang sama, kemudian melakukan encoding userID dan melakukan proses encoding angka ke ke userID
"""

user_ids = df['UserID'].unique().tolist()
 
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
 
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""Pada tahap ini kita mengubah ISBN menjadi list tanpa nilai yang sama, kemudian melakukan encoding ISBN dan melakukan proses encoding angka ke ISBN"""

isbn = df['ISBN'].unique().tolist()
 
book_to_book_encoded = {x: i for i, x in enumerate(isbn)}
 
book_encoded_to_book = {i: x for i, x in enumerate(isbn)}

"""**Memetakan userID dan placeID ke dataframe yang berkaitan**

Lakukan mapping userID ke dataframe user dan mapping ISBN ke dataframe book
"""

df['user'] = df['UserID'].map(user_to_user_encoded)
 
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""**Mengecek jumlah data dan mengubah nilai rating**

Mengecek jumlah data seperti jumlah user, jumlah buku, kemudian mengubah nilai rating menjadi float. Dapatkan hasil maximum dan minimum rating
"""

num_users = len(user_to_user_encoded)
print(num_users)
 
num_book = len(book_encoded_to_book)
print(num_book)
 
df['rating'] = df['BookRating'].values.astype(np.float32)
 
min_rating = min(df['rating'])
 
max_rating = max(df['rating'])
 
print('Jumlah User : {}, Jumlah Buku: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""## Membagi Data untuk Training dan Validasi

**Mengacak dataset**

Pada tahap ini kita akan mengacak dataset, dengan menset random_state=42
"""

df = df.sample(frac=1, random_state=42)
df

"""**Membagi dataset**

Membuat variabel x untuk mencocokkan data user dan buku menjadi satu value, kemudian membuat variabel y untuk membuat rating dari hasil lalu membagi menjadi 80% data train dan 20% data validasi
"""

x = df[['user', 'book']].values
 
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""## Proses Training"""

class RecommenderNet(tf.keras.Model):
 

  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( 
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) 
    self.book_embedding = layers.Embedding( 
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) 
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) 
    user_bias = self.user_bias(inputs[:, 0]) 
    book_vector = self.book_embedding(inputs[:, 1]) 
    book_bias = self.book_bias(inputs[:, 1]) 
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""**Model compile**

Lakukan compile pada model yang sudah didefinisikan dan gunakan metrik RMSE
"""

model = RecommenderNet(num_users, num_book, 50) 
 
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""**Memulai training**

Lakukan training model dengan epoch 10 dan batch_size 128
"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""## Visualisasi Metrik

Lakukan plot metrik
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi Buku

Mengambil sampel user secara acak dan definisikan variabel book_not_read yang merupakan daftar buku yang belum pernah dibaca
"""

book_df = book_new
df = pd.read_csv('/content/booksdataset/Ratings.csv')
 
user_id = df.UserID.sample(1).iloc[0]
book_read_by_user = df[df.UserID == user_id]
 
book_not_read = book_df[~book_df['id'].isin(book_read_by_user.ISBN.values)]['id'] 
book_not_read = list(
    set(book_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_read = [[book_to_book_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

"""Tahap di bawah ini akan menampilkan rekomendasi untuk pembaca"""

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_read[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('books with high ratings from user')
print('----' * 8)
 
top_book_user = (
    book_read_by_user.sort_values(
        by = 'BookRating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_name, ':', row.author)
 
print('----' * 8)
print('Top 10 books recommendation')
print('----' * 8)
 
recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.book_name, ':', row.author)